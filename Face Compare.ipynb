{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photo Matched\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "\n",
    "picture_of_me = face_recognition.load_image_file('F:/Data_Science/FaceRecognition/Hari_Adhar.jpg')\n",
    "my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n",
    "\n",
    "compare_pic = face_recognition.load_image_file('F:/Data_Science/FaceRecognition/0.jpg')\n",
    "compare_face_encoding = face_recognition.face_encodings(compare_pic)[0]\n",
    "\n",
    "results = face_recognition.compare_faces([my_face_encoding], compare_face_encoding)\n",
    "\n",
    "if results[0] == True:\n",
    "    print('Photo Matched')\n",
    "else:\n",
    "    print('No Match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces in the photo: 1\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py\n",
    "    \n",
    "image = face_recognition.load_image_file('F:/Data_Science/FaceRecognition/Hari_Adhar.jpg')\n",
    "image_landmarks = face_recognition.face_landmarks(image)\n",
    "\n",
    "print('Number of faces in the photo: {}'.format(len(image_landmarks)))\n",
    "\n",
    "for landmarks in image_landmarks:\n",
    "    pil_image = Image.fromarray(image)\n",
    "    d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    for facial_feature in landmarks.keys():\n",
    "        d.line(landmarks[facial_feature], width=5)\n",
    "\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying color vs black n white photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 1050, 3)\n"
     ]
    }
   ],
   "source": [
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "#Construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('i','--images', required=True, help = 'Path to input directory of images')\n",
    "ap.add_argument(\"-t\", \"--threshold\", type=float, default=100.0, \n",
    "                help=\"focus measures that fall below this value will be considered 'blurry'\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in paths.list_images(args[\"images\"]):\n",
    "    # load the image, convert it to grayscale, and compute the\n",
    "    # focus measure of the image using the Variance of Laplacian\n",
    "    # method\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = variance_of_laplacian(gray)\n",
    "    text = \"Not Blurry\"\n",
    " \n",
    "    # if the focus measure is less than the supplied threshold,\n",
    "    # then the image should be considered \"blurry\"\n",
    "    if fm < args[\"threshold\"]:\n",
    "        text = \"Blurry\"\n",
    " \n",
    "    # show the image\n",
    "    cv2.putText(image, \"{}: {:.2f}\".format(text, fm), (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    key = cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.sbhamidipati.com/how-i-detect-blurry-images-using-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 7.7531671356964855\n",
      "Blurred Image\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "threshold = 100\n",
    "\n",
    "img = cv2.imread('F:/Data_Science/FaceRecognition/Blurr_1.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "score = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "print('Score: {}'.format(score))\n",
    "if score > threshold:\n",
    "    print('Clean Image')\n",
    "else:\n",
    "    print('Blurred Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
